# [실습] Python을 활용한 AI 모델링 - 머신러닝 파트
+ 이번시간에는 Python을 활용한 AI 모델링에서 머신러닝에 대해 실습해 보겠습니다.
+ 머신러닝 모델에는 아래와 같이 모델들이 있습니다.
 + 단일 분류예측 모델 : LogisticRegression, KNN, DecisionTree
 + 앙상블(Ensemble) 모델 : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending

+ 솔직히, 머신러닝이 딥러닝보다 코딩하기 쉽습니다. 4줄 템플릿에 맞쳐 코딩하면 되기 때문입니다.
+ 그래도, 한가지 당부 드리고 싶은 말은 "백문이불여일타" 입니다. 
+ 이론보다 실습이 더 많은 시간과 노력이 투자 되어야 합니다.

## 학습목차
1. 실습을 위한 KT AIDU 환경변수 설정
2. 머신러닝 모델 프로세스
 - 데이터 가져오기
 - 데이터 전처리
 - Train, Test 데이터셋 분할
 - 데이터 정규화
 - 단일 분류예측 모델 : LogisticRegression, KNN, DecisionTree
 - 앙상블(Ensemble) 모델 : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending


# 
# 1. 실습을 위한 KT AIDU 환경변수 설정


```python
# 코드실행시 경고 메시지 무시

import warnings
warnings.filterwarnings(action='ignore') 
```

##### AIDU 라이브러리 임포트


```python
from aicentro.session import Session
from aicentro.framework.keras import Keras as AiduFrm

aidu_session = Session(verify=False)
aidu_framework = AiduFrm(session=aidu_session)
```

##### AIDU 디렉토리 환경변수

- data 경로:         aidu_framework.config.data_dir


```python
# aidu_framework.config.data_dir 내용 확인
aidu_framework.config.data_dir
```




    '/aihub/data'



# 
# 2. 머신러닝 모델 프로세스
① 라이브러리 임포트(import)  
② 데이터 가져오기(Loading the data)  
③ 탐색적 데이터 분석(Exploratory Data Analysis)  
④ 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 
더미특성 생성, 특성 추출 (feature engineering) 등  
⑤ Train, Test  데이터셋 분할  
⑥ 데이터 정규화(Normalizing the Data)  
⑦ 모델 개발(Creating the Model)  
⑧ 모델 성능 평가

## ① 라이브러리 임포트

##### 필요 라이브러리 임포트


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## ② 데이터 로드

#### cust_data.csv 파일 컬럼명
+ 고객등급(cust_class), 성별(sex_type), 나이(age), 사용서비스수(efct_svc_count), 서비스중지여부 (dt_stop_yn), 미납여부(npay_yn)
+ 3개월 평균 요금(r3m_avg_bill_amt), A서비스 3개월 평균요금(r3m_A_avg_arpu_amt), B서비스 3개월 평균요금(r3m_B_avg_arpu_amt), 해지여부(termination_yn)


```python
# cust_data.csv 파일 읽기
df = pd.read_csv('cust_data.csv')
```

## ③ 데이터 분석


```python
# 12컬럼, 7814 라인
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB



```python
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>sex</th>
      <th>age</th>
      <th>service</th>
      <th>stop</th>
      <th>npay</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>termination</th>
      <th>by_age</th>
      <th>bill_rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7809</th>
      <td>C</td>
      <td>M</td>
      <td>76</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>1860.0000</td>
      <td>1716.000000</td>
      <td>0.0000</td>
      <td>N</td>
      <td>75</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7810</th>
      <td>C</td>
      <td>F</td>
      <td>15</td>
      <td>1</td>
      <td>N</td>
      <td>Y</td>
      <td>1296.0999</td>
      <td>194.414985</td>
      <td>643.1001</td>
      <td>N</td>
      <td>15</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7811</th>
      <td>G</td>
      <td>M</td>
      <td>12</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>13799.6666</td>
      <td>2069.949990</td>
      <td>10605.9266</td>
      <td>N</td>
      <td>10</td>
      <td>midhigh</td>
    </tr>
    <tr>
      <th>7812</th>
      <td>C</td>
      <td>F</td>
      <td>40</td>
      <td>0</td>
      <td>N</td>
      <td>N</td>
      <td>3140.0000</td>
      <td>942.000000</td>
      <td>1884.0000</td>
      <td>Y</td>
      <td>40</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7813</th>
      <td>C</td>
      <td>F</td>
      <td>59</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>2436.9000</td>
      <td>365.535000</td>
      <td>1839.9000</td>
      <td>N</td>
      <td>55</td>
      <td>low</td>
    </tr>
  </tbody>
</table>
</div>




```python
# termination 레이블 불균형 
df['termination'].value_counts().plot(kind='bar')
```




    <AxesSubplot:>




    
![png](output_18_1.png)
    


## ④ 데이터 전처리

+ Object 컬럼에 대해 Pandas get_dummies 함수 활용하여 One-Hot-Encoding


```python
cal_cols = ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating']
```


```python
df1 = pd.get_dummies(data=df, columns=cal_cols, drop_first=True)
```


```python
# 19컬럼, 7814 라인
df1.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 19 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   age                  7814 non-null   int64  
     1   service              7814 non-null   int64  
     2   avg_bill             7814 non-null   float64
     3   A_bill               7814 non-null   float64
     4   B_bill               7814 non-null   float64
     5   by_age               7814 non-null   int64  
     6   class_D              7814 non-null   uint8  
     7   class_E              7814 non-null   uint8  
     8   class_F              7814 non-null   uint8  
     9   class_G              7814 non-null   uint8  
     10  class_H              7814 non-null   uint8  
     11  sex_M                7814 non-null   uint8  
     12  stop_Y               7814 non-null   uint8  
     13  npay_Y               7814 non-null   uint8  
     14  termination_Y        7814 non-null   uint8  
     15  bill_rating_low      7814 non-null   uint8  
     16  bill_rating_lowmid   7814 non-null   uint8  
     17  bill_rating_mid      7814 non-null   uint8  
     18  bill_rating_midhigh  7814 non-null   uint8  
    dtypes: float64(3), int64(3), uint8(13)
    memory usage: 465.6 KB


## ⑤ Train, Test  데이터셋 분할


```python
from sklearn.model_selection import train_test_split
```


```python
X = df1.drop('termination_Y', axis=1).values
y = df1['termination_Y'].values
```


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.3, 
                                                    stratify=y,
                                                    random_state=42)
```


```python
X_train.shape
```




    (5469, 18)




```python
y_train.shape
```




    (5469,)



## ⑥ 데이터 정규화/스케일링(Normalizing/Scaling)


```python
# 숫자 분포 이루어진 컬럼 확인
df1.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>service</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>by_age</th>
      <th>class_D</th>
      <th>class_E</th>
      <th>class_F</th>
      <th>class_G</th>
      <th>class_H</th>
      <th>sex_M</th>
      <th>stop_Y</th>
      <th>npay_Y</th>
      <th>termination_Y</th>
      <th>bill_rating_low</th>
      <th>bill_rating_lowmid</th>
      <th>bill_rating_mid</th>
      <th>bill_rating_midhigh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7809</th>
      <td>76</td>
      <td>1</td>
      <td>1860.0000</td>
      <td>1716.000000</td>
      <td>0.0000</td>
      <td>75</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7810</th>
      <td>15</td>
      <td>1</td>
      <td>1296.0999</td>
      <td>194.414985</td>
      <td>643.1001</td>
      <td>15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7811</th>
      <td>12</td>
      <td>1</td>
      <td>13799.6666</td>
      <td>2069.949990</td>
      <td>10605.9266</td>
      <td>10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7812</th>
      <td>40</td>
      <td>0</td>
      <td>3140.0000</td>
      <td>942.000000</td>
      <td>1884.0000</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7813</th>
      <td>59</td>
      <td>1</td>
      <td>2436.9000</td>
      <td>365.535000</td>
      <td>1839.9000</td>
      <td>55</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.preprocessing import MinMaxScaler
```


```python
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```


```python
X_train[:2], y_train[:2]
```




    (array([[0.38      , 0.33333333, 0.4295439 , 0.06384702, 0.41944434,
             0.4       , 0.        , 0.        , 0.        , 0.        ,
             1.        , 1.        , 0.        , 1.        , 0.        ,
             0.        , 0.        , 1.        ],
            [0.58      , 0.11111111, 0.20111297, 0.38498933, 0.        ,
             0.6       , 1.        , 0.        , 0.        , 0.        ,
             0.        , 1.        , 0.        , 0.        , 0.        ,
             1.        , 0.        , 0.        ]]),
     array([0, 0], dtype=uint8))



+ 모델 입력갯수, 출력갯수 확인


```python
X_train.shape
```




    (5469, 18)




```python
y_train.shape
```




    (5469,)



## ⑦ 모델 개발

#### 모델별 바차트 그려주고 성능 확인을 위한 함수


```python
# 모델별로 Accuracy 점수 저장
# 모델 Accuracy 점수 순서대로 바차트를 그려 모델별로 성능 확인 가능

from sklearn.metrics import accuracy_score

my_predictions = {}

colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',
          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',
          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 
          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',
          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 
          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'
         ]

# 모델명, 예측값, 실제값을 주면 위의 plot_predictions 함수 호출하여 Scatter 그래프 그리며
# 모델별 MSE값을 Bar chart로 그려줌
def accuracy_eval(name_, pred, actual):
    global predictions
    global colors

    plt.figure(figsize=(12, 9))

    acc = accuracy_score(actual, pred)
    my_predictions[name_] = acc * 100

    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)
    
    df = pd.DataFrame(y_value, columns=['model', 'accuracy'])
    print(df)
   
    length = len(df)
    
    plt.figure(figsize=(10, length))
    ax = plt.subplot()
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['model'], fontsize=15)
    bars = ax.barh(np.arange(len(df)), df['accuracy'])
    
    for i, v in enumerate(df['accuracy']):
        idx = np.random.choice(len(colors))
        bars[i].set_color(colors[idx])
        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')
        
    plt.title('accuracy', fontsize=18)
    plt.xlim(0, 100)
    
    plt.show()
```

###  
### 1) 로지스틱 회귀 (LogisticRegression, 분류)


```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report
```


```python
lg = LogisticRegression(C=1.0,max_iter=2000)
lg.fit(X_train, y_train)
```




    LogisticRegression(max_iter=2000)




```python
# 분류기 성능 평가(score)
lg.score(X_test, y_test)
```




    0.929637526652452



- 분류기 성능 평가 지표


```python
lg_pred = lg.predict(X_test)
```


```python
# 오차행렬
# TN  FP
# FN  TP

confusion_matrix(y_test, lg_pred) 
```




    array([[2098,   11],
           [ 154,   82]])




```python
# 정확도 : 굉장히 높다
accuracy_score(y_test, lg_pred)  
```




    0.929637526652452




```python
# 정밀도
precision_score(y_test, lg_pred) 
```




    0.8817204301075269




```python
# 재현율 : 굉장히 낮다.
recall_score(y_test, lg_pred)  
```




    0.3474576271186441




```python
# 정밀도 + 재현율
f1_score(y_test, lg_pred) 
```




    0.4984802431610942




```python
print(classification_report(y_test, lg_pred))
```

                  precision    recall  f1-score   support
    
               0       0.93      0.99      0.96      2109
               1       0.88      0.35      0.50       236
    
        accuracy                           0.93      2345
       macro avg       0.91      0.67      0.73      2345
    weighted avg       0.93      0.93      0.92      2345
    



```python
accuracy_eval('LogisticRegression', lg_pred, y_test)
```

                    model   accuracy
    0  LogisticRegression  92.963753



    <Figure size 864x648 with 0 Axes>



    
![png](output_53_2.png)
    


###   
### 2) KNN (K-Nearest Neighbor)


```python
from sklearn.neighbors import KNeighborsClassifier
```


```python
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
```




    KNeighborsClassifier()




```python
knn_pred = knn.predict(X_test)
```


```python
accuracy_eval('K-Nearest Neighbor', knn_pred, y_test)
```

                    model   accuracy
    0  K-Nearest Neighbor  94.712154
    1  LogisticRegression  92.963753



    <Figure size 864x648 with 0 Axes>



    
![png](output_58_2.png)
    


###  
### 3) 결정트리(DecisionTree)


```python
from sklearn.tree import DecisionTreeClassifier
```


```python
dt = DecisionTreeClassifier(max_depth=10, random_state=42)
dt.fit(X_train, y_train)
```




    DecisionTreeClassifier(max_depth=10, random_state=42)




```python
dt_pred = dt.predict(X_test)
```


```python
accuracy_eval('DecisionTree', dt_pred, y_test)
```

                    model   accuracy
    0        DecisionTree  97.313433
    1  K-Nearest Neighbor  94.712154
    2  LogisticRegression  92.963753



    <Figure size 864x648 with 0 Axes>



    
![png](output_63_2.png)
    


### 
### **앙상블 기법의 종류**
- 배깅 (Bagging): 여러개의 DecisionTree 활용하고 샘플 중복 생성을 통해 결과 도출. RandomForest
- 부스팅 (Boosting): 약한 학습기를 순차적으로 학습을 하되, 이전 학습에 대하여 잘못 예측된 데이터에 가중치를 부여해 오차를 보완해 나가는 방식. XGBoost, LGBM
- 스태킹 (Stacking): 여러 모델을 기반으로 예측된 결과를 통해 Final 학습기(meta 모델)이 다시 한번 예측

![앙상블](https://teddylee777.github.io/images/2019-12-18/image-20191217144823555.png)

###  
### 4) 랜덤포레스트(RandomForest)
+ Bagging 대표적인 모델로써, 훈련셋트를 무작위로 각기 다른 서브셋으로 데이터셋을 만들고
+ 여러개의 DecisonTree로 학습하고 다수결로 결정하는 모델

**주요 Hyperparameter**
- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- max_depth: 깊어질 수 있는 최대 깊이. 과대적합 방지용
- n_estimators: 앙상블하는 트리의 갯수
- max_features: 최대로 사용할 feature의 갯수. 과대적합 방지용
- min_samples_split: 트리가 분할할 때 최소 샘플의 갯수. default=2. 과대적합 방지용


```python
from sklearn.ensemble import RandomForestClassifier
```


```python
rfc = RandomForestClassifier(n_estimators=3, random_state=42)
rfc.fit(X_train, y_train)
```




    RandomForestClassifier(n_estimators=3, random_state=42)




```python
rfc_pred = rfc.predict(X_test)
```


```python
accuracy_eval('RandomForest Ensemble', rfc_pred, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1           DecisionTree  97.313433
    2     K-Nearest Neighbor  94.712154
    3     LogisticRegression  92.963753



    <Figure size 864x648 with 0 Axes>



    
![png](output_71_2.png)
    


###  
### 5) XGBoost
+ 여러개의 DecisionTree를 결합하여 Strong Learner 만드는 Boosting 앙상블 기법
+ Kaggle 대회에서 자주 사용하는 모델이다.

**주요 특징**
- scikit-learn 패키지가 아닙니다.
- 성능이 우수함
- GBM보다는 빠르고 성능도 향상되었습니다.
- 여전히 학습시간이 매우 느리다

**주요 Hyperparameter**
- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1
- n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100
- max_depth: 트리의 깊이. 과대적합 방지용. default=3. 
- subsample: 샘플 사용 비율. 과대적합 방지용. default=1.0
- max_features: 최대로 사용할 feature의 비율. 과대적합 방지용. default=1.0


```python
!pip install xgboost
```

    Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)
    Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.5.4)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m



```python
from xgboost import XGBClassifier
```


```python
xgb = XGBClassifier(n_estimators=3, random_state=42)  # 10초 소요
xgb.fit(X_train, y_train)
```




    XGBClassifier(n_estimators=3, random_state=42)




```python
xgb_pred = xgb.predict(X_test)
```


```python
accuracy_eval('XGBoost', xgb_pred, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1                XGBoost  97.611940
    2           DecisionTree  97.313433
    3     K-Nearest Neighbor  94.712154
    4     LogisticRegression  92.963753



    <Figure size 864x648 with 0 Axes>



    
![png](output_79_2.png)
    


###  
### 6) Light GBM
+ XGBoost와 함께 주목받는 DecisionTree 알고리즘 기반의 Boosting 앙상블 기법
+ XGBoost에 비해 학습시간이 짧은 편이다.

**주요 특징**
- scikit-learn 패키지가 아닙니다.
- 성능이 우수함
- 속도도 매우 빠릅니다.

**주요 Hyperparameter**
- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1
- n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100
- max_depth: 트리의 깊이. 과대적합 방지용. default=3. 
- colsample_bytree: 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0


```python
!pip install lightgbm
```

    Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.3.0)
    Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.5.4)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.19.5)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.24.2)
    Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (3.1.0)
    Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (1.1.0)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m



```python
from lightgbm import LGBMClassifier
```


```python
lgbm = LGBMClassifier(n_estimators=3, random_state=42)   # 1분 소요
lgbm.fit(X_train, y_train)
```




    LGBMClassifier(n_estimators=3, random_state=42)




```python
lgbm_pred = lgbm.predict(X_test)
```


```python
accuracy_eval('LGBM', lgbm_pred, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1                XGBoost  97.611940
    2           DecisionTree  97.313433
    3     K-Nearest Neighbor  94.712154
    4     LogisticRegression  92.963753
    5                   LGBM  89.936034



    <Figure size 864x648 with 0 Axes>



    
![png](output_87_2.png)
    


### 7) Stacking

개별 모델이 예측한 데이터를 기반으로 **final_estimator** 종합하여 예측을 수행합니다.
- 성능을 극으로 끌어올릴 때 활용하기도 합니다.
- 과대적합을 유발할 수 있습니다. (특히, 데이터셋이 적은 경우)


```python
from sklearn.ensemble import StackingRegressor, StackingClassifier
```


```python
stack_models = [
    ('LogisticRegression', lg), 
    ('KNN', knn), 
    ('DecisionTree', dt),
]
```


```python
# stack_models로 선언된 모델(LogisticRegression,KNN,DecisionTree)의 예측결과를 최종 meta_model(final_estimator)을 RandomForest(rfc) 사용하여 분류 예측 
stacking = StackingClassifier(stack_models, final_estimator=rfc, n_jobs=-1)
```


```python
stacking.fit(X_train, y_train)   # 1분 20초 소요
```




    StackingClassifier(estimators=[('LogisticRegression',
                                    LogisticRegression(max_iter=2000)),
                                   ('KNN', KNeighborsClassifier()),
                                   ('DecisionTree',
                                    DecisionTreeClassifier(max_depth=10,
                                                           random_state=42))],
                       final_estimator=RandomForestClassifier(n_estimators=3,
                                                              random_state=42),
                       n_jobs=-1)




```python
stacking_pred = stacking.predict(X_test)
```


```python
accuracy_eval('Stacking Ensemble', stacking_pred, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1                XGBoost  97.611940
    2           DecisionTree  97.313433
    3      Stacking Ensemble  96.247335
    4     K-Nearest Neighbor  94.712154
    5     LogisticRegression  92.963753
    6                   LGBM  89.936034



    <Figure size 864x648 with 0 Axes>



    
![png](output_95_2.png)
    


### 8) Weighted Blending

각 모델의 예측값에 대하여 weight를 곱하여 최종 output 계산
- 모델에 대한 가중치를 조절하여, 최종 output을 산출합니다.
- **가중치의 합은 1.0**이 되도록 합니다.


```python
final_outputs = {
    'DecisionTree': dt_pred, 
    'randomforest': rfc_pred, 
    'xgb': xgb_pred, 
    'lgbm': lgbm_pred,
    'stacking': stacking_pred,
}
```


```python
final_prediction=\
final_outputs['DecisionTree'] * 0.1\
+final_outputs['randomforest'] * 0.2\
+final_outputs['xgb'] * 0.25\
+final_outputs['lgbm'] * 0.15\
+final_outputs['stacking'] * 0.3\
```


```python
# 가중치 계산값이 0.5 초과하면 1, 그렇지 않으면 0
final_prediction = np.where(final_prediction > 0.5, 1, 0)
```


```python
accuracy_eval('Weighted Blending', final_prediction, y_test)
```

                       model   accuracy
    0                XGBoost  97.825160
    1      Weighted Blending  97.697228
    2  RandomForest Ensemble  97.611940
    3           DecisionTree  97.313433
    4      Stacking Ensemble  96.247335
    5     K-Nearest Neighbor  94.712154
    6     LogisticRegression  92.963753
    7                   LGBM  89.936034



    <Figure size 864x648 with 0 Axes>



    
![png](output_101_2.png)
    



```python

```


```python

```

## 배운 내용 정리
1. 머신러닝 모델 프로세스 <br>
① 라이브러리 임포트(import)  
② 데이터 가져오기(Loading the data)  
③ 탐색적 데이터 분석(Exploratory Data Analysis)  
④ 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 
더미특성 생성, 특성 추출 (feature engineering) 등  
⑤ Train, Test  데이터셋 분할  
⑥ 데이터 정규화(Normalizing the Data)  
⑦ 모델 개발(Creating the Model)  
⑧ 모델 성능 평가
2. 평가 지표 활용 : 모델별 성능 확인을 위한 함수 (가져다 쓰면 된다)
3. 단일 회귀예측 모델 : LogisticRegression, KNN, DecisionTree
4. 앙상블 (Ensemble) : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending
